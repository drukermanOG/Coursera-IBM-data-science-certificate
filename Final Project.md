**The problem**

Even the greatest cities in the world lack a certain balance when it comes to the geographical distribution of their citizens. There are always those neighborhoods where everybody wants to live in, and the real estate is highly valued. On the other hand, there are neighborhoods that are less populated and usually considered as the choice of those who cannot afford the former. This imbalance creates a variance in population density, making some parts of the city overcrowded while others have plenty of space – a major problem when it comes to infrastructure utilization and investments.

Although there are many forces in play here (among which are geographical properties, social economics, pollution etc), one of the key factors is the level of services\goods available in the neighborhood. We all need some essentials in our day-to-day life, and we need them close by, preferably in a walking distance. It’s true that we can take a bus or the metro and get there after spending another 20 min (if the public transport is efficient). However, with people being so busy nowadays and so used to instant digital response – a service or a product offered even 20 min away might be too demanding.

Now you might think to yourself, “this is a free market we’re talking about! private businesses will seize the opportunity of having some more revenue if they only get the chance to do so. If they are not expanding their operations to these neighborhoods, it means they see no potential there”. There’s some truth in this saying but the problem is a bit more complex in my opinion and suffer from the ‘chicken and egg’ effect. Not enough people -> services are not extended -> not enough people in that place.
If we would have been able to list those core services and products and segment the different neighborhoods according to what is missing in them - It would have been possible to offer that information to various stakeholders. They, in turn, can Influence the neighborhoods’ value and attract more citizens to it.

*Stakeholders*

Municipalities are the primary stakeholder in this project. Knowing what exactly makes a neighborhood less desirable is the key in turning it around. For example, if a coffee shop is missing - a tax benefit can be initiated; if it’s a park that’s missing - the city can sponsor the construction. Naturally, making a neighborhood more desirable is of primary importance to the city. Another stake holder is the private business. Knowing what’s missing can give a business an edge in earning customer’s loyalty ahead of competition. Again, if it’s a potential coffee shop the neighborhood lacks, Starbucks would be happy to know that. Once more services and products become available, the wheel will start turning and the chicken & egg cycle will be broken. This happens naturally all the time, all around the world. Data can make it happen faster.

**Data**

The city of choice is Toronto. I will be using the same public dataset downloaded from Wikipedia in week 3 peer-graded assignment. That, in order to map all neighborhoods in the city and their GPS coordinates. Each row in the dataset lists a neighborhood, the borough\post-code it belongs to and its coordinates. For example, Rough neighborhood is located in Scarborough under post-code M1B. I will process and enrich this data later on to use it as samples for clustering, since the problem is that of grouping neighborhoods according to the services around them.

The complementary information that is missing are the different services offered in every neighborhood. Here, Foursquare free engine will provide the requirement. It will list every relevant venue, its category and the coordinates. The categories labels are crucial to the project since it will not only determine the value of each sample (borough\post code \ neighborhood) but will allow us to study the variety of categories existing in the city and determine which ones are to be considered as critical services. As an example, we see that M1B post-code has only one venue close to it and it belong to the “restaurants” category: Wendy’s at (43.807448, -79.199056). This is basically the DNA of M1B. Also, since restaurants of all kinds are the most popular category in the city, with more than 500 of them around, it’s clear that this needs to be considered as a crucial service.

**Methodology**

After downloading the public data of Toronto’s neighborhoods, I am facing with the most fundamental question: what the basic building block in terms of data. Meaning, what data points will I feed the clustering algorithms with? the answer is comprised of two components:

 * a.	The geographical resolution – Borough or Post code or Neighborhood?
 * b.	The service resolution – Venue or Venue category?
  
*Exploratory data*

A further data analysis is required to determine this. After cleaning the data and removing invalid entries, it is can be seen that in terms of geographical resolution, there are 11 boroughs, 103 post codes and several hundreds of neighborhoods overall. Intuitively, boroughs are too big & few to choose while neighborhoods are too many. Therefore, the natural candidate is the post code. Taking a look at the ratio of neighborhoods to post codes strengthens the assumption. Most post codes include 1-3 neighborhoods, making it a good choice since borough to post code ratio is much lower (~1:10) and there are too many neighborhoods to receive significant results. Especially when aiming for minimal scanning distance in which case each neighborhood will have very few services around it.

As for the service resolution, here the statistics is clearer. When scanning post codes using a 500-meter radius (my definition of short walking distance) and having a 150 venues per post code limit, foursquare generates 2,234 venues in 274 categories. The choice is clear. I choose the venue categories as the service resolution. Moreover, since I am looking for essential services only, I need to reduce category amount even further. I did this by converting all restaurant types (52) into one category and merging similar services such as coffee shops and Café or Gym and Gym fitness clubs. On top of that I’ve set a threshold to eliminate relatively rare services which have less than 15% availability in the average post code. After conducting further manual picking of the 10 most essential services, the final data point vector looks as follows:

*(Post code, Restaurant, Park, Hotel, Grocery shop, Pub, Pharmacy, Bank, Coffee, Pastries, Workout)*

As for the values of the vector, it’s the proportion of a given service category out of the total amount of services in the post code. All proportions in a given post code sum up to 1, making them also probabilities of encountering the specific service. One final interesting statistic before I begin with the algorithms is the amount of services in each post code. The statistics show that there’s a great variance between different post codes. There are those with about 100 services in them while there are also ones with less than 5. This is important to notice since the entire business problem revolves around the argument that the city is imbalanced in terms of geographical and service distribution. 

*Machine learning techniques*

I started off with K means, being one of the most popular and efficient clustering algorithms out there in use. The number of clusters was determined using the silhouette score due to the fact that the popular elbow method is not reliable enough and does not address the distance to the cluster center, only the between-group variance. The silhouette score was high enough for around 10 cluster-target and therefore the parameter was set to 10 (it’s symbolic, could have been set to 11 or 9 easily).

Despite its popularity, K means has some drawbacks, the primary of which is the Euclidean distance metric and its use in the kernel function. The data is split halfway between cluster means leading to overall “softer” clustering. Meaning, weaker correlation between data points within each cluster. This might show the big picture, but - too vaguely. Knowing this, I turned next to run some density-based algorithms. DBSCAN was the first choice. It works differently in concept, advancing from point to point and looking to maximize intra-cluster correlation of data points without having predefined number of clusters to fill assign to. This should perform the clustering in a “harder” manner if the min points per cluster parameter is low enough (which I made sure happens through trial and error). “Harder” means that the common denominator in each group is distinct, allowing for focused conclusions to be drawn.

The second choice in density-based algorithms was Mean Shift. I picked it because I wanted to neutralize the input parameters as much as possible. I might have wrongly chosen them in DBSCAN after all. While DBSCAN requires both a distance (ε) and a minimal amount of points to form a dense region, Mean Shift needs only the distance to be used by the kernel function. That way, I will utilize the density approach with minimal setup intervention.

After applying K means and two density algorithms, I should explore another direction combining both. On the one hand, we need to run a "soft” clustering algorithm to be able to group data points with weaker correlation and see the big picture. On the other, we need to be able to form clusters with distinct patterns in them – whatever size they are. The chosen algorithm is GMM (Gaussian Mixture Model), providing both flexibility in terms of cluster shape and also being inherently structured. This can be seen as generalizing K means clustering to also address the covariance structure of the data. That way, if there are significantly different patterns in different parts of the data, the clustering process will recognize this and group accordingly. 

One last machine learning technique I decided to use is hierarchical agglomerative clustering. It gives a bit of a different insight. It will show the entire “structure” of the data, building it bottom up and step by step, allowing to monitor the process. I will set the cluster target to 10, as before, and the linkage type to “complete” such that intra-cluster correlation will be maximized again.
